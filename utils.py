"""
utils - æ•°æ®åˆ†ææ™ºèƒ½ä½“ä½¿ç”¨çš„å·¥å…·å‡½æ•°

Author: éª†æ˜Š
Version: 0.1
Date: 2025/6/25
"""
import json
import pandas as pd
import openpyxl
import io
import streamlit as st
import hashlib

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain.callbacks.base import BaseCallbackHandler

class StreamlitCallbackHandler(BaseCallbackHandler):
    """Streamlitæµå¼è¾“å‡ºå›è°ƒå¤„ç†å™¨"""
    
    def __init__(self, container):
        self.container = container
        self.text = ""
        self.step_count = 0
        
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """å¤„ç†æ–°çš„token"""
        self.text += token
        self.container.markdown(self.text + "â–Œ")
        
    def on_llm_end(self, response, **kwargs) -> None:
        """LLMç»“æŸæ—¶çš„å¤„ç†"""
        self.container.markdown(self.text)
        
    def on_agent_action(self, action, **kwargs) -> None:
        """Agentæ‰§è¡ŒåŠ¨ä½œæ—¶çš„å›è°ƒ"""
        self.step_count += 1
        self.container.info(f"ğŸ”„ æ‰§è¡Œæ­¥éª¤ {self.step_count}: {action.tool}")
        
    def on_tool_start(self, serialized, input_str: str, **kwargs) -> None:
        """å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶çš„å›è°ƒ"""
        tool_name = serialized.get("name", "æœªçŸ¥å·¥å…·")
        self.container.info(f"ğŸ› ï¸ æ­£åœ¨ä½¿ç”¨å·¥å…·: {tool_name}")
        
    def on_tool_end(self, output: str, **kwargs) -> None:
        """å·¥å…·æ‰§è¡Œå®Œæˆæ—¶çš„å›è°ƒ"""
        self.container.success(f"âœ… å·¥å…·æ‰§è¡Œå®Œæˆ")

PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œä½ çš„å›åº”å†…å®¹å–å†³äºç”¨æˆ·çš„è¯·æ±‚å†…å®¹ï¼Œè¯·æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤å¤„ç†ç”¨æˆ·è¯·æ±‚ï¼š
1. æ€è€ƒé˜¶æ®µ (Thought) ï¼šå…ˆåˆ†æç”¨æˆ·è¯·æ±‚ç±»å‹ï¼ˆæ–‡å­—å›ç­”/è¡¨æ ¼/å›¾è¡¨ï¼‰ï¼Œå¹¶éªŒè¯æ•°æ®ç±»å‹æ˜¯å¦åŒ¹é…ã€‚
2. è¡ŒåŠ¨é˜¶æ®µ (Action) ï¼šæ ¹æ®åˆ†æç»“æœé€‰æ‹©ä»¥ä¸‹ä¸¥æ ¼å¯¹åº”çš„æ ¼å¼ã€‚
   - çº¯æ–‡å­—å›ç­”:
     {"answer": "ä¸è¶…è¿‡50ä¸ªå­—ç¬¦çš„æ˜ç¡®ç­”æ¡ˆ"}

   - è¡¨æ ¼æ•°æ®ï¼š
     {"table":{"columns":["åˆ—å1", "åˆ—å2", ...], "data":[["ç¬¬ä¸€è¡Œå€¼1", "å€¼2", ...], ["ç¬¬äºŒè¡Œå€¼1", "å€¼2", ...]]}}

   - æŸ±çŠ¶å›¾
     {"bar":{"columns": ["A", "B", "C", ...], "data":[35, 42, 29, ...]}}

   - æŠ˜çº¿å›¾
     {"line":{"columns": ["A", "B", "C", ...], "data": [35, 42, 29, ...]}}

   - æ•£ç‚¹å›¾
     {"scatter":{"x_data": [1, 2, 3, ...], "y_data": [4, 5, 6, ...], "labels": ["ç‚¹1", "ç‚¹2", ...]}}

   - é¥¼å›¾
     {"pie":{"labels": ["ç±»åˆ«1", "ç±»åˆ«2", ...], "values": [30, 45, 25, ...]}}

   - çƒ­åŠ›å›¾
     {"heatmap":{"data": [[1, 2, 3], [4, 5, 6]], "x_labels": ["A", "B", "C"], "y_labels": ["X", "Y"]}}
     
3. æ ¼å¼æ ¡éªŒè¦æ±‚
   - å­—ç¬¦ä¸²å€¼å¿…é¡»ä½¿ç”¨è‹±æ–‡åŒå¼•å·
   - æ•°å€¼ç±»å‹ä¸å¾—æ·»åŠ å¼•å·
   - ç¡®ä¿æ•°ç»„é—­åˆæ— é—æ¼
   é”™è¯¯æ¡ˆä¾‹ï¼š{'columns':['Product', 'Sales'], data:[[A001, 200]]}
   æ­£ç¡®æ¡ˆä¾‹ï¼š{"columns":["product", "sales"], "data":[["A001", 200]]}

æ³¨æ„ï¼šå“åº”æ•°æ®çš„"output"ä¸­ä¸è¦æœ‰æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ä»¥åŠå…¶ä»–æ ¼å¼ç¬¦å·ã€‚

å½“å‰ç”¨æˆ·è¯·æ±‚å¦‚ä¸‹ï¼š\n"""


def generate_cache_key(df, query):
    """ç”Ÿæˆç¼“å­˜é”®"""
    # ä½¿ç”¨æ•°æ®æ¡†çš„å½¢çŠ¶ã€åˆ—åå’ŒæŸ¥è¯¢å†…å®¹ç”Ÿæˆå”¯ä¸€é”®
    # å®‰å…¨åœ°å¤„ç†æ•°æ®ç±»å‹ï¼Œé¿å…åºåˆ—åŒ–é”™è¯¯
    try:
        dtype_dict = {col: str(dtype) for col, dtype in df.dtypes.to_dict().items()}
    except Exception:
        dtype_dict = {col: "unknown" for col in df.columns}
    
    df_info = f"{df.shape}_{list(df.columns)}_{dtype_dict}"
    cache_string = f"{df_info}_{query}"
    return hashlib.md5(cache_string.encode()).hexdigest()

@st.cache_data(ttl=3600)  # ç¼“å­˜1å°æ—¶
def cached_dataframe_analysis(_df, query, cache_key):
    """ç¼“å­˜çš„æ•°æ®åˆ†æå‡½æ•°"""
    return _perform_analysis(_df, query)

def _perform_analysis(df, query):
    """æ‰§è¡Œå®é™…çš„æ•°æ®åˆ†æ"""
    load_dotenv()
    import os
    model = ChatOpenAI(
        base_url='https://oneapi.xty.app/v1',
        api_key=os.getenv("OPENAI_API_KEY"),
        model='gpt-4o-mini',
        temperature=0,
        max_tokens=8192,
        streaming=False  # å…³é—­æµå¼è¾“å‡ºï¼Œç¡®ä¿æ ¼å¼ä¸€è‡´æ€§
    )
    agent = create_pandas_dataframe_agent(
        llm=model,
        df=df,
        agent_executor_kwargs={"handle_parsing_errors": True},
        max_iterations=32,
        allow_dangerous_code=True,
        verbose=True
    )

    prompt = PROMPT_TEMPLATE + query

    try:
        response = agent.invoke({"input": prompt})
        # å¢å¼ºJSONè§£æé”™è¯¯å¤„ç†
        try:
            return json.loads(response["output"])
        except json.JSONDecodeError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºç­”æ¡ˆ
            return {"answer": response["output"]}
    except Exception as err:
        print(f"åˆ†æé”™è¯¯: {err}")
        return {"answer": "æš‚æ—¶æ— æ³•æä¾›åˆ†æç»“æœï¼Œè¯·ç¨åé‡è¯•ï¼"}

def dataframe_agent(df, query, stream_container=None):
    """æ•°æ®åˆ†æä»£ç†å‡½æ•°ï¼Œæ”¯æŒç¼“å­˜å’Œæµå¼è¾“å‡º"""
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = generate_cache_key(df, query)
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
    if stream_container is not None:
        return dataframe_agent_streaming(df, query, stream_container)
    else:
        # ä½¿ç”¨ç¼“å­˜
        return cached_dataframe_analysis(df, query, cache_key)

def dataframe_agent_streaming(df, query, stream_container):
    """æ”¯æŒæµå¼è¾“å‡ºçš„æ•°æ®åˆ†æå‡½æ•°"""
    load_dotenv()
    import os
    
    # åˆ›å»ºæµå¼å›è°ƒå¤„ç†å™¨
    callback_handler = StreamlitCallbackHandler(stream_container)
    
    model = ChatOpenAI(
        base_url="https://api.openai-hk.com/v1",
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o-mini",
        temperature=0,
        max_tokens=8192,
        streaming=False,  # å…³é—­æ¨¡å‹çº§åˆ«çš„æµå¼è¾“å‡ºï¼Œé¿å…æ ¼å¼è§£æé—®é¢˜
        callbacks=[callback_handler]
    )
    agent = create_pandas_dataframe_agent(
        llm=model,
        df=df,
        agent_executor_kwargs={"handle_parsing_errors": True},
        max_iterations=32,
        allow_dangerous_code=True,
        verbose=True
    )

    prompt = PROMPT_TEMPLATE + query

    try:
        # æ˜¾ç¤ºåˆ†æå¼€å§‹ä¿¡æ¯
        stream_container.info("ğŸ¤– AIæ­£åœ¨åˆ†ææ‚¨çš„æ•°æ®...")
        response = agent.invoke({"input": prompt})
        
        # å°è¯•è§£æJSONå“åº”
        try:
            result = json.loads(response["output"])
            stream_container.success("âœ… åˆ†æå®Œæˆï¼")
            return result
        except json.JSONDecodeError as json_err:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬
            stream_container.warning("âš ï¸ å“åº”æ ¼å¼è§£æå¼‚å¸¸ï¼Œè¿”å›åŸå§‹ç»“æœ")
            return {"answer": response["output"]}
            
    except Exception as err:
        error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(err)}"
        print(error_msg)
        stream_container.error(error_msg)
        return {"answer": "æš‚æ—¶æ— æ³•æä¾›åˆ†æç»“æœï¼Œè¯·ç¨åé‡è¯•ï¼"}


def load_data_file(uploaded_file, file_type_option):
    """
    åŠ è½½ä¸åŒæ ¼å¼çš„æ•°æ®æ–‡ä»¶
    
    Args:
        uploaded_file: Streamlitä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
        file_type_option: ç”¨æˆ·é€‰æ‹©çš„æ–‡ä»¶ç±»å‹é€‰é¡¹
    
    Returns:
        pandas.DataFrame æˆ–åŒ…å«å¤šä¸ªå·¥ä½œè¡¨çš„å­—å…¸
    """
    file_extension = uploaded_file.name.split('.')[-1].lower()
    
    try:
        if file_type_option.startswith("Excel"):
            # Excelæ–‡ä»¶å¤„ç†ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
            try:
                wb = openpyxl.load_workbook(uploaded_file)
                if len(wb.sheetnames) > 1:
                    # å¤šä¸ªå·¥ä½œè¡¨ï¼Œè¿”å›å­—å…¸
                    sheets = {}
                    for sheet_name in wb.sheetnames:
                        try:
                            df = pd.read_excel(uploaded_file, sheet_name=sheet_name)
                            if not df.empty:  # åªæ·»åŠ éç©ºå·¥ä½œè¡¨
                                sheets[sheet_name] = df
                        except Exception as e:
                            print(f"è·³è¿‡å·¥ä½œè¡¨ {sheet_name}: {str(e)}")
                    
                    if not sheets:
                        raise ValueError("æ‰€æœ‰å·¥ä½œè¡¨éƒ½æ— æ³•è¯»å–æˆ–ä¸ºç©º")
                    return {"sheets": sheets}
                else:
                    # å•ä¸ªå·¥ä½œè¡¨
                    return pd.read_excel(uploaded_file)
            except Exception as e:
                raise ValueError(f"Excelæ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
                
        elif file_type_option.startswith("CSV"):
            # CSVæ–‡ä»¶å¤„ç†ï¼Œæ·»åŠ ç¼–ç æ£€æµ‹å’Œé”™è¯¯å¤„ç†
            try:
                return pd.read_csv(uploaded_file, encoding='utf-8')
            except UnicodeDecodeError:
                try:
                    return pd.read_csv(uploaded_file, encoding='gbk')
                except UnicodeDecodeError:
                    return pd.read_csv(uploaded_file, encoding='latin-1')
            
        elif file_type_option.startswith("JSON"):
            # JSONæ–‡ä»¶å¤„ç†ï¼Œæ”¯æŒå¤šç§JSONç»“æ„
            content = uploaded_file.read()
            if isinstance(content, bytes):
                content = content.decode('utf-8')
            
            try:
                json_data = json.loads(content)
            except json.JSONDecodeError as e:
                raise ValueError(f"JSONæ ¼å¼é”™è¯¯: {str(e)}")
            
            # å°è¯•ä¸åŒçš„JSONç»“æ„
            if isinstance(json_data, list):
                if len(json_data) == 0:
                    raise ValueError("JSONæ•°ç»„ä¸ºç©º")
                return pd.DataFrame(json_data)
            elif isinstance(json_data, dict):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°æ®æ•°ç»„
                for key, value in json_data.items():
                    if isinstance(value, list) and len(value) > 0:
                        if isinstance(value[0], dict):
                            return pd.DataFrame(value)
                
                # å¦‚æœæ˜¯åµŒå¥—å­—å…¸ï¼Œå°è¯•normalize
                try:
                    return pd.json_normalize(json_data)
                except Exception:
                    # å¦‚æœnormalizeå¤±è´¥ï¼Œå°†å­—å…¸è½¬æ¢ä¸ºå•è¡ŒDataFrame
                    return pd.DataFrame([json_data])
            else:
                raise ValueError("ä¸æ”¯æŒçš„JSONæ ¼å¼ï¼Œè¯·ç¡®ä¿JSONåŒ…å«æ•°ç»„æˆ–å¯¹è±¡ç»“æ„")
                
        elif file_type_option.startswith("TSV"):
            # TSVæ–‡ä»¶å¤„ç†
            return pd.read_csv(uploaded_file, sep='\t')
            
        elif file_type_option.startswith("Parquet"):
            # Parquetæ–‡ä»¶å¤„ç†
            return pd.read_parquet(uploaded_file)
            
        elif file_type_option.startswith("TXT"):
            # TXTæ–‡ä»¶å¤„ç†ï¼ˆå‡è®¾æ˜¯åˆ†éš”ç¬¦åˆ†éš”çš„æ•°æ®ï¼‰
            content = uploaded_file.read()
            if isinstance(content, bytes):
                content = content.decode('utf-8')
            
            # å°è¯•æ£€æµ‹åˆ†éš”ç¬¦
            lines = content.strip().split('\n')
            if len(lines) < 2:
                raise ValueError("TXTæ–‡ä»¶å†…å®¹ä¸è¶³ï¼Œæ— æ³•è§£æä¸ºè¡¨æ ¼æ•°æ®")
            
            # æ”¹è¿›çš„åˆ†éš”ç¬¦æ£€æµ‹ç®—æ³•
            separators = [',', '\t', ';', '|', ' ']
            best_sep = ','
            max_consistency = 0
            
            # æ£€æŸ¥å‰å‡ è¡Œæ¥ç¡®å®šæœ€ä¸€è‡´çš„åˆ†éš”ç¬¦
            sample_lines = lines[:min(10, len(lines))]
            
            for sep in separators:
                col_counts = []
                for line in sample_lines:
                    if line.strip():  # è·³è¿‡ç©ºè¡Œ
                        col_counts.append(len(line.split(sep)))
                
                if col_counts:
                    # è®¡ç®—åˆ—æ•°çš„ä¸€è‡´æ€§ï¼ˆç›¸åŒåˆ—æ•°çš„è¡Œæ•°ï¼‰
                    most_common_cols = max(set(col_counts), key=col_counts.count)
                    consistency = col_counts.count(most_common_cols)
                    
                    # é€‰æ‹©ä¸€è‡´æ€§æœ€é«˜ä¸”åˆ—æ•°å¤§äº1çš„åˆ†éš”ç¬¦
                    if consistency > max_consistency and most_common_cols > 1:
                        max_consistency = consistency
                        best_sep = sep
            
            # ä½¿ç”¨StringIOæ¥æ¨¡æ‹Ÿæ–‡ä»¶å¯¹è±¡ï¼Œæ·»åŠ é”™è¯¯å¤„ç†å‚æ•°
            string_io = io.StringIO(content)
            try:
                return pd.read_csv(
                    string_io, 
                    sep=best_sep,
                    on_bad_lines='skip',  # è·³è¿‡æœ‰é—®é¢˜çš„è¡Œ
                    engine='python',     # ä½¿ç”¨Pythonå¼•æ“ï¼Œæ›´å®½å®¹
                    skipinitialspace=True  # è·³è¿‡åˆ†éš”ç¬¦åçš„ç©ºæ ¼
                )
            except Exception as csv_error:
                # å¦‚æœCSVè§£æå¤±è´¥ï¼Œå°è¯•ä½œä¸ºå›ºå®šå®½åº¦æ–‡ä»¶å¤„ç†
                string_io = io.StringIO(content)
                try:
                    return pd.read_fwf(string_io)
                except Exception:
                    raise ValueError(f"æ— æ³•è§£æTXTæ–‡ä»¶æ ¼å¼ã€‚åŸå§‹é”™è¯¯: {str(csv_error)}")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type_option}")
            
    except Exception as e:
        raise Exception(f"æ–‡ä»¶è§£æé”™è¯¯: {str(e)}")
